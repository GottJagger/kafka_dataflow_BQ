{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, GoogleCloudOptions\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "from apache_beam.io.kafka import ReadFromKafka\n",
    "\n",
    "# Define opciones de pipeline\n",
    "dataflow_options =[\"--project=gcp-test-chaki\",\n",
    "                    \"--region=us-central1\",\n",
    "                    \"--runner=DataflowRunner\",\n",
    "                    \"--temp_location=gs://bucket-kafka-test/temp/\",\n",
    "                    \"--job_name=test-job1\",\n",
    "                    \"--worker_machine_type=n1-standard-1\",\n",
    "                    \"--num_workers=1\"]\n",
    "options = PipelineOptions(dataflow_options)\n",
    "gcloud = options.view_as(GoogleCloudOptions)\n",
    "\n",
    "# Define opciones de configuraci贸n\n",
    "setup_options = options.view_as(SetupOptions)\n",
    "setup_options.setup_file = 'setup.py'\n",
    "\n",
    "# Define opciones de Kafka\n",
    "kafka_options = {\n",
    "    \"bootstrap_servers\": \"127.0.0.1:9092\",  # Direcci贸n IP y puerto de Kafka\n",
    "    \"topics\": [\"test_topic\"],  # Nombre del topic de Kafka\n",
    "    \"consumer_config\": {\"group.id\": \"my-group\"}  # Configuraci贸n de consumidor de Kafka\n",
    "}\n",
    "\n",
    "# Define una funci贸n para procesar mensajes de Kafka\n",
    "def process_message(message):\n",
    "    # Decodifica el mensaje\n",
    "    message_str = message.value.decode('utf-8')\n",
    "\n",
    "    # Carga el mensaje como un objeto JSON\n",
    "    message_json = json.loads(message_str)\n",
    "\n",
    "    # Procesa el mensaje\n",
    "    # En este ejemplo, simplemente imprimimos el mensaje en la consola\n",
    "    print(message_json)\n",
    "\n",
    "# Define el pipeline\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    # Lee datos de Kafka utilizando el transform ReadFromKafka\n",
    "    # Este transform lee datos de Kafka en formato PCollection\n",
    "    kafka_data = (p\n",
    "                  | \"ReadFromKafka\" >> ReadFromKafka(**kafka_options)\n",
    "                  | \"ProcessData\" >> beam.Map(process_message))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version #2 of apache beam using dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "from apache_beam.io.external.kafka import ReadFromKafka\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "\n",
    "\n",
    "# DATAFLOW CONFIGURATION (pipeline options)\n",
    "dataflow_options = [\n",
    "    '--project=gcp-test-chaki',\n",
    "    '--region=us-central1',\n",
    "    '--runner=DataflowRunner',\n",
    "    '--temp_location=gs://bucket-kafka-test/temp/',\n",
    "    '--job_name=test-job1',\n",
    "    '--worker_machine_type=n1-standard-1',\n",
    "    '--num_workers=1'\n",
    "]\n",
    "\n",
    "options = PipelineOptions(dataflow_options)\n",
    "gcloud_options = options.view_as(GoogleCloudOptions)\n",
    "\n",
    "\n",
    "# KAFKA CONFIGURATION\n",
    "kafka_topic = 'my-kafka-topic'\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "kafka_read_time = None  # Change to datetime.datetime(2022, 1, 1) to read from a specific time\n",
    "\n",
    "kafka_consumer_config = {\n",
    "    'bootstrap.servers': kafka_bootstrap_servers,\n",
    "    'group.id': 'my-group-id'\n",
    "}\n",
    "\n",
    "\n",
    "# BIGQUERY CONFIGURATION\n",
    "project_id = 'gcp-test-chaki'\n",
    "dataset_id = 'test_kafka_dataflow'\n",
    "table_id = 'my-bigquery-table'\n",
    "\n",
    "table_spec = f'{project_id}:{dataset_id}.{table_id}'\n",
    "\n",
    "\n",
    "# IMPLEMENTACION DEL PIPELINE\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    lines = p | 'Read from Kafka' >> ReadFromKafka(\n",
    "        consumer_config=kafka_consumer_config,\n",
    "        topics=[kafka_topic],\n",
    "        start_read_time=kafka_read_time,\n",
    "        # Use 'stop_read_time' to read up to a specific timestamp\n",
    "    )\n",
    "\n",
    "    lines | 'Write to BigQuery' >> WriteToBigQuery(\n",
    "        table_spec,\n",
    "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "        project=project_id\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
